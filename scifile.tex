\documentclass[11pt]{article}
\usepackage{scicite}

% Default packages
\usepackage{amsmath, amsfonts, amssymb, amsthm, epsfig, epstopdf, titling, url, array}
\usepackage{algorithm} 
\usepackage{algorithmic} 
\usepackage{graphicx}
% Because it is convenient
\usepackage{hyperref}

% To insert tables
\usepackage{array}

% We might need some code insertion
\usepackage{minted}

% Also indent first paragraph of a section
\usepackage{indentfirst}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}



\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm

\newenvironment{sciabstract}{
\begin{quote} \bf}
{\end{quote}}
\renewcommand\refname{References and Notes}

\newcounter{lastnote}
\newenvironment{scilastnote}{\setcounter{lastnote}{\value{enumiv}}
\addtocounter{lastnote}{+1}
\begin{list}
  {\arabic{lastnote}.}
  {\setlength{\leftmargin}{.22in}}
  {\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Wikipedia Recommender System with serendipity} 


\author
    {
      Barszezak Yoann, Bricout Rapha\"el, David Nicolas, Dupr\'e R\'emi,\\
      Fouch\'e Aziz, Gourdel Garance, Kaddar Younesse, Mallem Maher,\\
      \\
      \normalsize{Department of Computer science, ENS Paris-Saclay}\\
    }

 

    \date{November 2017}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document} 

% Double-space the manuscript.

\baselineskip10pt

% Make the title.

\maketitle 



% Place your abstract within the special {sciabstract} environment.

\begin{sciabstract}
  TO DO : Abstract
  Younesse : Overleaf - Github sync test
\end{sciabstract}




\section{Introduction}


\section{Problem Overview}




\section{Retrivial of Candidate articles}

\subsection{Neighbourhood}

Let's try to understand the idea of proximity between two articles. One way to do it is to introduce a potential function telling us the proximity between two articles.

\subsubsection{Naive Similarity}

Our attempt to define this potential function uses the set of categories linked to an article, it is called the similarity.





\vspace*{5mm}
\begin{defn}
  Let $a_1$ and $a_2$ be two articles. We call $C_1$ (resp. $C_2$) the set of categories linked to $a_1$ (resp. $a_2$).
  We define the similarity of $a_1$ and $a_2$ the following quantity:\\
  \begin{equation*}
    S_W(a_1,a_2) = \frac{Card(C_1 \cap C_2)}{min(Card(C_1),Card(C_2))}
  \end{equation*}
\end{defn}

\begin{rem}
  $\forall a_1\: a_2,\; S_W(a_1,a_2) \in [0,1]$
\end{rem}

\vspace*{5mm}
With this definition in mind, let's seek for an output of the ``Retrivial of Candidate Articles''.


Let's call $a_c$ the current article.
Given an subinterval $I$ of $[0,1]$ (define in the serendipity subsection), one way to find candidate articles will be to pick randomly $N$ articles $(a_i)_{1 \leq i \leq N}$ such that:
\begin{center}
  $\forall i \; S_W(a_c,a_i) \in I$
\end{center}

\vspace*{5mm}
This approach should work as long as the similarity is precise (in the case of wikipedia, it means as long as an article have a significant number of categories). Unfortunately, some articles are poorly cateorised. Thus, it is not always accurate to use this object.\\
Therefore, an other approach is needed.






\subsubsection{Ontology basis}

An other way to solve this problem is to use an Ontology on the wikipedia articles.

\vspace*{5mm}
\begin{defn}
  Let $A$ a set of theme. We call $\mathbb{O}$ an Ontology over $A$ a directed tree in which :
  \begin{itemize}
  \item $ Node(\mathbb{O}) = A$
  \item if $t_1$ is a sub-theme of $t_2$ then $(t_1,t_2) \in Edge(\mathbb{O})$
  \end{itemize}
\end{defn}

\vspace*{5mm}

\begin{rem}
  The relation ``is a sub-theme'' is not transitive.
\end{rem}

\begin{figure}[!h]
	\centering
    \includegraphics[scale = 0.3]{ExOntology.pdf}
    \caption{Ontology over number}
\end{figure}

This kind of strucure is available on a YAGO software (Yet An Other Great Ontology). Thus, the building of such an ontology on wikipedia is not required. In the following, we will suppose that every theme in YAGO matches with an article on wikipedia. \\
Now that we have in mind the definition of ontology, let's try to find a way of using it to select articles.\\


\begin{algorithm}
  \caption{Calculate $A^*$ the selected articles}
  \begin{algorithmic}
    \REQUIRE $wish \in \{specific , general\}$ , $p_1 \: ,p_2 \in ]0,1[ $, $N \in \mathbb{N}$ , $a_c \in A$
    \STATE $A^* \leftarrow \emptyset$
    \WHILE {$ Card(A) \leq N $}
    \IF {$wish = specific$}
    \STATE $ r \leftarrow geometric.random(p_1)$
    \STATE Ask YAGO a for a random son of depth r from $a_c$
    \STATE $ A^* \leftarrow A^* \cup \{a_{YAGO}\}$
    \ELSE
    \STATE $ r \leftarrow geometric.random(p_2)$
    \STATE Ask YAGO a for a random father of depth r from $a_c$
    \STATE $ A^*  \leftarrow A^* \cup \{a_{YAGO}\}$ 
    \ENDIF
    \ENDWHILE
    \RETURN $A^*$
  \end{algorithmic}
\end{algorithm}

-choix de p1 et p2 dans serandipity

\newpage
\subsection{serendipity}

\subsubsection{Similarity}

-choix de I

\subsubsection{Ontology}

-parcours de l'Ontology

\section{Candidate Ranking}

\subsection{possible input in neural network}
\subsubsection{Category vector}
\subsubsection{Word2Vect and Wikidata}

\subsection{Wide and deep Neural Network}
\subsubsection{Wide : memorization/focus}
\subsubsection{Deep : generalization/serendipity}


\newpage

\section{System architecture}

First, we give a global diagram to expose interactions between the user interface, \textit{Wikipedia} pages, modules and \textit{Wikipedia}'s APIs. 

\begin{figure}[h!]
	\centering
    \includegraphics[width=400pt]{diagram.png}
    \caption{Global software's structure}
    \label{arch_glo}
\end{figure}

Blue arrows represent system's input, orange arrow is for the output. Green arrows explains how data is exchanged internally between the different modules. Arrowed labeling is defined as follows :
\begin{itemize}
\item \textbf{1)} \textit{Extraction of all links mentioned in the article (without the links of header, side, and footer)}.
\item \textbf{2)} \textit{Visit of article neighborhood (it may be 2-hop neighboring) to enhance serendipity, selection of best neighbors based on general criteria}.
\item \textbf{3)} \textit{Selected articles are injected into the recommendation module}.
\item \textbf{4), 5)} \textit{Recommendation module feeds the user-specific neural network with selected articles to sort them}.
\item \textbf{6)} \textit{Best articles are re-injected into the page, to be recommended to the user}.
\item \textbf{7)} \textit{User gives its feedback on proposed articles, reactions are stored locally}.
\item \textbf{8)} \textit{Thanks to user feedback, network structure is updated to fit user's interests}.
\end{itemize}

\subsection{Zoom on recommendation method}

\begin{figure}[h!]
	\centering
    \includegraphics[width=400pt]{diagram_zoom.png}
    \caption{How recommendation method works}
    \label{arch_glo}
\end{figure}

\subsubsection{Triple filtering}

The main challenge which has been to be faced is the HTTP requests' efficiency problem. Indeed, the first idea is to get all page links, visit them all and sort them using some pertinence criteria. It was nevertheless technically impossible ; due to the size of HTTP requests, even performed asynchronously, the answer time was huge. \\

The solution developed was to introduce a triple filtering, which consists in three passes going more and more complex and time-expensive. It allowed the module to be able to explore pages to \textit{2-hops} from the original page

\subsubsection{Iterative triple filtering}

\subsection{User interface}
We started the implementation of an in-browser plugin for wikipedia suggestion.
The goal of this plugin is to insert an interface in any wikipedia page that allows the user to give his opinion about this page. The plugin will keep an user profile up to date and suggest wikipedia pages to the user that it considers relevant for this user.

As our goal is to introduce serendipity in such a system, we also added a cursor to parametrize a level of serendipity. Bigger this parameter is set, the less the user will be suggested pages about a topic he already eared of.

\subsection{Wikipedia API's}
% a few words about what both api provide


\section{Experimentation}
\subsection{Similarity}
Using wikipedia's official api, we were able to calculate the naive similarity between two pages defined at \ref{definition:S_w}. For example, the following code will prompt $0.6364$, the similarity processed between the page about Jimi Hendrix (of id 16095) and Michael Jackson (of id 14995351).
  \mint{javascript}|apiMod.distance(16095, 14995351, console.log); // "similarity(Michael J.,  Jimi H.)"|
  
Thus, we were able to experiment this distance over a set of webpages. We can thus qualitatively test this algorithm. It is not really hard to get examples where this calculation is unrelevant (for example, Michael Jackson is closer to France than Paris, cf. Figure \ref{fig:similarity}), but still, on many examples, it gives a trend that fits with the intuition.

\begin{figure}
    \caption{Comparison of some pages of wikipedia (cf definition \ref{definition:S_w})}
    \label{fig:similarity}
	\centering
	\begin{tabular}{c|c|c}
		Page 1			&	Page 2			&	Similarity \\
		\hline\hline
		Michael Jackson	&	Jimi Hendrix	&	0.6364	\\
		Michael Jackson	&	France			&	0.8600	\\
		Paris			&	France			&	0.6842	\\
		Luxembourg		&	France			&	0.5416	\\
  \end{tabular}
\end{figure}


\section{State of the Art}



\bibliography{scibib}

\bibliographystyle{Science}

\end{document}
